{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karou1182001/NLPAssignments/blob/main/Assignment3/Intro_to_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuInr3Sk0J3C"
      },
      "source": [
        "# Assignment 3: Intro to Recurrent Neural Networks: Math, Training, and the Copy Task\n",
        "\n",
        "# Instructor: Dr. Ankur Mali\n",
        "# University of South Florida (Spring 2025)\n",
        "### In this tutorial we will build RNNs based on equation and will compare 3 popular frameworks (Jax, TensorFlow and Pytorch)\n",
        "\n",
        "## Vanilla RNN -- For more in depth explanation refer to your slides\n",
        "\n",
        "### Forward Pass (Inference) -- Stage 1\n",
        "Given an input at time \\(t\\):\n",
        "\\begin{aligned}\n",
        "\\mathbf{x}_t \\in \\mathbb{R}^{d_{\\text{in}}},\\quad \\mathbf{h}_{t-1} \\in \\mathbb{R}^{d_{\\text{hid}}}\n",
        "\\end{aligned}\n",
        "we define RNN parameters:\n",
        "\\begin{aligned}\n",
        "\\mathbf{W}_x \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{hid}}}, \\quad\n",
        "\\mathbf{W}_h \\in \\mathbb{R}^{d_{\\text{hid}} \\times d_{\\text{hid}}}, \\quad\n",
        "\\mathbf{b}_h \\in \\mathbb{R}^{d_{\\text{hid}}}.\n",
        "\\end{aligned}\n",
        "\n",
        "The hidden state update:\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_t = \\tanh\\Bigl(\\mathbf{x}_t\\,\\mathbf{W}_x \\;+\\;\\mathbf{h}_{t-1}\\,\\mathbf{W}_h \\;+\\;\\mathbf{b}_h\\Bigr).\n",
        "\\end{aligned}\n",
        "\n",
        "Over a sequence  ($\\mathbf{x}_1$, $\\dots$, $\\mathbf{x}_T$), we unroll:\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_0 = \\mathbf{0},\\quad\n",
        "\\mathbf{h}_1 = \\tanh(\\mathbf{x}_1 \\mathbf{W}_x + \\mathbf{h}_0 \\mathbf{W}_h + \\mathbf{b}_h),\\,\\dots,\\,\n",
        "\\mathbf{h}_T = \\tanh(\\mathbf{x}_T \\mathbf{W}_x + \\mathbf{h}_{T-1} \\mathbf{W}_h + \\mathbf{b}_h).\n",
        "\\end{aligned}\n",
        "\n",
        "Optionally, each hidden state  \\($\\mathbf{h}_t$\\) can be projected to the output dimension $d_{\\text{in}}$:\n",
        "\\begin{aligned}\n",
        "\\mathbf{\\hat{y}}_t = \\mathbf{h}_t \\mathbf{W}_{\\text{out}} + \\mathbf{b}_{\\text{out}}\n",
        "\\end{aligned}\n",
        "\n",
        "<!-- $\\mathbf{\\hat{y}}$_t = $\\mathbf{h}_t$,$\\mathbf{W}_{\\text{out}}$ + $\\mathbf{b}_{\\text{out}}$. -->\n",
        "\n",
        "\n",
        "### Remaining Stages\n",
        "We define a loss (Stage 2) over all time steps, for instance:\n",
        "\\begin{aligned}\n",
        "\\mathbf{L} = \\frac{1}{T} \\sum_{t=1}^T \\left\\|\\,\\mathbf{\\hat{y}}_t - \\mathbf{y}_t\\,\\right\\|^2,\n",
        "\\end{aligned}\n",
        "and use Backpropagation Through Time (BPTT) (Stage 3). An optimizer (e.g., Adam) updates parameters (Stage 4):\n",
        "\\begin{aligned}\n",
        "\\theta \\,\\leftarrow\\, \\theta \\;-\\; \\eta \\,\\nabla_\\theta \\,\\mathbf{L}.\n",
        "\\end{aligned}\n",
        "\n",
        "---\n",
        "\n",
        "## GRU\n",
        "\n",
        "### Forward Pass (Inference)\n",
        "A Gated Recurrent Unit includes reset $\\mathbf{r}_t$ and update $\\mathbf{z}_t$ gates:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mathbf{z}_t &= \\sigma\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_z + \\mathbf{h}_{t-1}\\,\\mathbf{U}_z + \\mathbf{b}_z\\bigr), \\\\\n",
        "\\mathbf{r}_t &= \\sigma\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_r + \\mathbf{h}_{t-1}\\,\\mathbf{U}_r + \\mathbf{b}_r\\bigr), \\\\\n",
        "\\tilde{\\mathbf{h}}_t &= \\tanh\\!\\bigl(\\mathbf{x}_t \\mathbf{W}_h + (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1})\\,\\mathbf{U}_h + \\mathbf{b}_h\\bigr), \\\\\n",
        "\\mathbf{h}_t &= (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} \\;+\\; \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t.\n",
        "\\end{aligned}\n",
        "\n",
        "where $\\sigma$ is the sigmoid function, and $\\odot$ denotes elementwise multiplication.\n",
        "\n",
        "### Remaining Stages\n",
        "As in the vanilla RNN, define a loss $\\mathbf{L}$ (e.g. MSE). The same BPTT logic applies, but the derivatives now include the GRU gating operations. Parameters (e.g., $\\mathbf{W}_z, \\mathbf{U}_z, \\ldots$ ) are updated by any gradient-based optimizer.\n",
        "\n",
        "---\n",
        "\n",
        "## Optimizer\n",
        "A typical training loop includes:\n",
        "\n",
        "1. **Forward pass**: compute model outputs $\\mathbf{\\hat{y}}_t$.\n",
        "2. **Loss computation**: $\\mathbf{L}(\\mathbf{\\hat{y}}_t, \\mathbf{y}_t)$.\n",
        "3. **Backward pass**: compute $\\nabla_\\theta \\mathbf{L}$ via BPTT.\n",
        "4. **Parameter update**:\n",
        "   \\begin{aligned}\n",
        "   \\theta \\leftarrow \\theta - \\eta \\;\\nabla_\\theta \\,\\mathcal{L}.\n",
        "   \\end{aligned}\n",
        "   (For example, using Adam, SGD, RMSProp, etc.)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m03_Eo4PBeOM"
      },
      "source": [
        "\n",
        "# The Copy Task\n",
        "The **copy task** is a simple sequence-to-sequence challenge:\n",
        "\n",
        "- **Input**: a sequence of random vectors {$\\mathbf{x}_1, \\dots, \\mathbf{x}_T$}.\n",
        "- **Target**: the **same** sequence {$\\mathbf{x}_1, \\dots, \\mathbf{x}_T$}.\n",
        "\n",
        "Thus, the model should learn to produce $\\mathbf{\\hat{y}}_t \\approx \\mathbf{x}_t$ at each time step ($t$). It's a straightforward yet revealing test of a model’s capacity to retain and reproduce a sequence—particularly sensitive to the model’s ability to **remember** information over time.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6lcugDetLTx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, lax\n",
        "import time\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "########################################\n",
        "# Custom RNN Cell (Core Computation)\n",
        "########################################\n",
        "\n",
        "# ------- PyTorch Single-Step RNN Cell -------\n",
        "class RNNCellPyTorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in PyTorch.\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # For a single step:  input: (batch_size, input_size)\n",
        "        #                    hidden: (batch_size, hidden_size)\n",
        "        self.W_x = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.W_h = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_h = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        # x_t: [batch_size, input_size]\n",
        "        # h_prev: [batch_size, hidden_size]\n",
        "        h_t = torch.tanh(x_t @ self.W_x + h_prev @ self.W_h + self.b_h)\n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level PyTorch RNN that unrolls over time -------\n",
        "class RNNPyTorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Unrolls the RNNCell over a full sequence.\n",
        "    Also includes an output projection from hidden_size -> input_size\n",
        "    so we can do an MSE loss vs. the original input.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = RNNCellPyTorch(input_size, hidden_size)\n",
        "        # Output projection to match the original input dimension for copy task\n",
        "        self.W_out = torch.nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
        "        self.b_out = torch.nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size, seq_length, _ = X.shape\n",
        "        h = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]  # [batch_size, input_size]\n",
        "            h = self.rnn_cell(x_t, h)  # [batch_size, hidden_size]\n",
        "            # Project hidden -> input_size\n",
        "            out_t = h @ self.W_out + self.b_out\n",
        "            outputs.append(out_t.unsqueeze(1))  # shape [batch_size,1,input_size]\n",
        "        # Concatenate across time\n",
        "        return torch.cat(outputs, dim=1)  # [batch_size, seq_length, input_size]\n",
        "\n",
        "########################################\n",
        "# TensorFlow Implementation\n",
        "########################################\n",
        "\n",
        "# ------- Single-Step RNN Cell -------\n",
        "class RNNCellTF(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in TensorFlow.\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_x = self.add_weight(\n",
        "            shape=(input_size, hidden_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.W_h = self.add_weight(\n",
        "            shape=(hidden_size, hidden_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b_h = self.add_weight(\n",
        "            shape=(hidden_size,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, x_t, h_prev):\n",
        "        h_t = tf.math.tanh(\n",
        "            tf.matmul(x_t, self.W_x) + tf.matmul(h_prev, self.W_h) + self.b_h\n",
        "        )\n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level TF RNN that unrolls over time -------\n",
        "class RNNTF(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = RNNCellTF(input_size, hidden_size)\n",
        "        # Output projection\n",
        "        self.W_out = self.add_weight(\n",
        "            shape=(hidden_size, input_size), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b_out = self.add_weight(\n",
        "            shape=(input_size,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, X):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size = tf.shape(X)[0]\n",
        "        seq_length = tf.shape(X)[1]\n",
        "        h = tf.zeros((batch_size, self.hidden_size), dtype=X.dtype)\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]\n",
        "            h = self.rnn_cell(x_t, h)\n",
        "            out_t = tf.matmul(h, self.W_out) + self.b_out\n",
        "            outputs.append(tf.expand_dims(out_t, axis=1))\n",
        "        return tf.concat(outputs, axis=1)  # [batch_size, seq_length, input_size]\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "# Training / Benchmark\n",
        "########################################\n",
        "\n",
        "# -------------- PyTorch Benchmark --------------\n",
        "def benchmark_pytorch(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    model = RNNPyTorch(input_size, hidden_size)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    start_time = time.time()\n",
        "\n",
        "    X_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_torch = torch.tensor(Y_train, dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_torch)  # [batch_size, seq_length, input_size]\n",
        "        loss = criterion(output, Y_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(f\"Epoch {epoch} | Loss torch: {loss.item():.6f}\")\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "# -------------- TensorFlow Benchmark --------------\n",
        "def benchmark_tensorflow(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    model = RNNTF(input_size, hidden_size)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    X_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    Y_tf = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(X_tf)\n",
        "            loss = loss_fn(output, Y_tf)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        #print(f\"Epoch {epoch} | Loss TF: {loss.numpy():.6f}\")\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return time.time() - start_time\n",
        "\n",
        "\n",
        "########################################\n",
        "# JAX Implementation -- Faster\n",
        "########################################\n",
        "\n",
        "def RNNCellJAX(params, x_t, h_prev):\n",
        "    W_x, W_h, b_h = params\n",
        "    h_t = jnp.tanh(jnp.dot(x_t, W_x) + jnp.dot(h_prev, W_h) + b_h)\n",
        "    return h_t\n",
        "\n",
        "def RNNJAX_unroll(params, X):\n",
        "    W_x, W_h, b_h, W_out, b_out = params\n",
        "    batch_size, seq_length, _ = X.shape\n",
        "    X_t = jnp.swapaxes(X, 0, 1)  # [seq_length, batch_size, input_size]\n",
        "\n",
        "    def step_fn(h_prev, x_t):\n",
        "        h_t = jnp.tanh(\n",
        "            jnp.dot(x_t, W_x)\n",
        "            + jnp.dot(h_prev, W_h)\n",
        "            + b_h\n",
        "        )\n",
        "        out_t = jnp.dot(h_t, W_out) + b_out\n",
        "        return h_t, out_t\n",
        "\n",
        "    h0 = jnp.zeros((batch_size, W_h.shape[0]))\n",
        "    final_h, outs = lax.scan(step_fn, h0, X_t)\n",
        "    # outs: [seq_length, batch_size, input_size]\n",
        "    outs = jnp.swapaxes(outs, 0, 1)  # [batch_size, seq_length, input_size]\n",
        "    return outs\n",
        "\n",
        "########################################\n",
        "# Training / Benchmark\n",
        "########################################\n",
        "\n",
        "def init_jax_params(key, input_size, hidden_size):\n",
        "    k1, k2, k3, k4, k5 = jax.random.split(key, 5)\n",
        "    W_x = 0.1 * jax.random.normal(k1, (input_size, hidden_size))\n",
        "    W_h = 0.1 * jax.random.normal(k2, (hidden_size, hidden_size))\n",
        "    b_h = jnp.zeros((hidden_size,))\n",
        "    W_out = 0.1 * jax.random.normal(k3, (hidden_size,  input_size))\n",
        "    b_out = jnp.zeros((input_size,))\n",
        "    return (W_x, W_h, b_h, W_out, b_out)\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    pred = RNNJAX_unroll(params, x)\n",
        "    return jnp.mean((pred - y) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, x, y, lr):\n",
        "    grads = jax.grad(loss_fn)(params, x, y)\n",
        "    new_params = []\n",
        "    for p, g in zip(params, grads):\n",
        "        new_params.append(p - lr * g)\n",
        "    return tuple(new_params)\n",
        "\n",
        "\n",
        "def benchmark_jax(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "    key = random.PRNGKey(42)\n",
        "    params = init_jax_params(key, input_size, hidden_size)\n",
        "\n",
        "    X_jax = jnp.array(X_train)\n",
        "    Y_jax = jnp.array(Y_train)\n",
        "\n",
        "    # warm-up to compile\n",
        "    _ = train_step(params, X_jax, Y_jax, lr)\n",
        "\n",
        "    start_time = time.time()\n",
        "    p = params\n",
        "    for epoch in range(epochs):\n",
        "        p = train_step(p, X_jax, Y_jax, lr)\n",
        "        #current_loss = loss_fn(p, X_jax, Y_jax)\n",
        "        #print(f\"Epoch {epoch} | Loss jax: {current_loss:.6f}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return total_time\n",
        "\n",
        "############################\n",
        "# Main Run\n",
        "############################\n",
        "def run_benchmark():\n",
        "    seq_length = 20\n",
        "    batch_size = 32\n",
        "    input_size = 10\n",
        "    hidden_size = 128\n",
        "    num_epochs = 10\n",
        "\n",
        "    np.random.seed(42)\n",
        "    X_train = np.random.rand(1000, seq_length, input_size).astype(np.float32)\n",
        "    Y_train = X_train.copy()\n",
        "\n",
        "    # PyTorch\n",
        "    pytorch_time = benchmark_pytorch(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "    # TensorFlow\n",
        "    tensorflow_time = benchmark_tensorflow(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "    # JAX\n",
        "    jax_time = benchmark_jax(input_size, hidden_size, X_train, Y_train, num_epochs)\n",
        "\n",
        "    print(f\"PyTorch Time: {pytorch_time:.4f} s\")\n",
        "    print(f\"TensorFlow Time: {tensorflow_time:.4f} s\")\n",
        "    print(f\"JAX Time: {jax_time:.4f} s\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Things to Learn\n",
        "\n",
        "########################################\n",
        "# JAX Implementation -- slow version -- This will work (Learn how to speedup things in JAX by comparing two implementation)\n",
        "########################################\n",
        "# def RNNCellJAX(params, x_t, h_prev):\n",
        "#     \"\"\"\n",
        "#     Single-step RNN cell in JAX.\n",
        "#     params = (W_x, W_h, b_h)\n",
        "#     h_t = tanh( x_t*W_x + h_prev*W_h + b )\n",
        "#     \"\"\"\n",
        "#     W_x, W_h, b_h = params\n",
        "#     h_t = jnp.tanh(jnp.dot(x_t, W_x) + jnp.dot(h_prev, W_h) + b_h)\n",
        "#     return h_t\n",
        "\n",
        "# def RNNJAX_unroll(params, X):\n",
        "#     \"\"\"\n",
        "#     Unroll RNNCellJAX across time.\n",
        "#     params_main = (W_x, W_h, b_h, W_out, b_out)\n",
        "#     X: [batch_size, seq_length, input_size]\n",
        "#     We'll swap so we scan over seq_length dimension.\n",
        "#     \"\"\"\n",
        "#     W_x, W_h, b_h, W_out, b_out = params\n",
        "#     batch_size, seq_length, _ = X.shape\n",
        "\n",
        "#     # Swap to [seq_length, batch_size, input_size]\n",
        "#     X_t = jnp.swapaxes(X, 0, 1)\n",
        "\n",
        "#     def step_fn(h_prev, x_t):\n",
        "#         h_t = jnp.tanh(jnp.dot(x_t, W_x) + jnp.dot(h_prev, W_h) + b_h)\n",
        "#         # Output projection back to input_size\n",
        "#         out_t = jnp.dot(h_t, W_out) + b_out\n",
        "#         return h_t, out_t\n",
        "\n",
        "#     h0 = jnp.zeros((batch_size, W_h.shape[0]))\n",
        "#     final_h, outs = lax.scan(step_fn, h0, X_t)\n",
        "#     # outs: [seq_length, batch_size, input_size]\n",
        "#     # we want [batch_size, seq_length, input_size], so swap axes\n",
        "#     outs = jnp.swapaxes(outs, 0, 1)\n",
        "#     return outs\n",
        "\n",
        "\n",
        "# -------------- JAX Benchmark -- slow version--------------\n",
        "# def init_jax_params(key, input_size, hidden_size):\n",
        "#     # W_x: [input_size, hidden_size]\n",
        "#     # W_h: [hidden_size, hidden_size]\n",
        "#     # b_h: [hidden_size]\n",
        "#     # W_out: [hidden_size, input_size]\n",
        "#     # b_out: [input_size]\n",
        "\n",
        "#     k1, k2, k3, k4, k5 = jax.random.split(key, 5)\n",
        "#     W_x = 0.1 * jax.random.normal(k1, (input_size, hidden_size))\n",
        "#     W_h = 0.1 * jax.random.normal(k2, (hidden_size, hidden_size))\n",
        "#     b_h = jnp.zeros((hidden_size,))\n",
        "#     W_out = 0.1 * jax.random.normal(k3, (hidden_size,  input_size))\n",
        "#     b_out = jnp.zeros((input_size,))\n",
        "#     return (W_x, W_h, b_h, W_out, b_out)\n",
        "\n",
        "# def benchmark_jax(input_size, hidden_size, X_train, Y_train, epochs=10, lr=0.01):\n",
        "#     key = random.PRNGKey(42)\n",
        "#     params = init_jax_params(key, input_size, hidden_size)\n",
        "\n",
        "#     def loss_fn(p, x, y):\n",
        "#         pred = RNNJAX_unroll(p, x)\n",
        "#         return jnp.mean((pred - y) ** 2)\n",
        "\n",
        "#     grad_fn = jax.grad(loss_fn)\n",
        "#     X_jax = jnp.array(X_train)\n",
        "#     Y_jax = jnp.array(Y_train)\n",
        "\n",
        "#     start_time = time.time()\n",
        "#     p = params\n",
        "#     for epoch in range(epochs):\n",
        "#         grads = grad_fn(p, X_jax, Y_jax)\n",
        "#         p = [param - lr*g for param, g in zip(p, grads)]\n",
        "\n",
        "#     return time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnUjvLTtUys",
        "outputId": "621bad37-ec61-4760-d66b-55eb1e88b5d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Time: 0.6485 s\n",
            "TensorFlow Time: 2.1559 s\n",
            "JAX Time: 0.0071 s\n"
          ]
        }
      ],
      "source": [
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "The goal of this project is to see how well different recurrent neural network architectures can handle the copy task with different sequence lengths. We’ll train the models on sequences of length **T=100** and use validation to find the best hyperparameters. But to really test how well the models generalize, we’ll also test them on longer sequences **(T=200, 500, 1000)** that they haven’t seen before. This will help us figure out if the models are truly learning the task or just memorizing the training data. By looking at how performance drops as the sequence length gets longer, we can find out which architectures are better at handling long term dependencies in sequential data."
      ],
      "metadata": {
        "id": "P743WOMa5Xn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Standard LSTM\n"
      ],
      "metadata": {
        "id": "Uj25X525BROn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Verifica si hay GPU disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")\n",
        "\n",
        "# Verifica qué GPU está disponible\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Prueba moviendo un tensor a la GPU\n",
        "x = torch.rand(1).to(device)\n",
        "print(f\"Tensor in {device}: {x}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtQYnZMKCor0",
        "outputId": "ec8ff1b9-06dd-4870-fd13-28261cdaa5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Tensor in cpu: tensor([0.0957])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.Implementation of the model"
      ],
      "metadata": {
        "id": "9ASGGgDfBYlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#------------------------------------------\n",
        "# PyTorch Standard LSTM\n",
        "#------------------------------------------\n",
        "\n",
        "# ------- Single-Step LSTM Cell\n",
        "class LSTMCellPyTorch(nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step LSTM cell in PyTorch\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)  # Input gate\n",
        "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)  # Forget gate\n",
        "        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)  # Output gate\n",
        "        self.W_c = nn.Linear(input_size + hidden_size, hidden_size)  # Candidate cell state\n",
        "\n",
        "    def forward(self, x_t, h_prev, c_prev):\n",
        "        \"\"\"\n",
        "        x_t: [batch_size, input_size]\n",
        "        h_prev: [batch_size, hidden_size]\n",
        "        c_prev: [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        combined = torch.cat((x_t, h_prev), dim=1)\n",
        "\n",
        "        i_t = torch.sigmoid(self.W_i(combined))\n",
        "        f_t = torch.sigmoid(self.W_f(combined))\n",
        "        o_t = torch.sigmoid(self.W_o(combined))\n",
        "        c_tilde = torch.tanh(self.W_c(combined))\n",
        "\n",
        "\n",
        "        c_t = f_t * c_prev + i_t * c_tilde\n",
        "        h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "        return h_t, c_t\n",
        "\n",
        "# ------- Higher-level PyTorch LSTM that unrolls over time\n",
        "class LSTMPyTorch(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = LSTMCellPyTorch(input_size, hidden_size)\n",
        "        # Output projection\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        X: [batch_size, seq_length, input_size]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        h_t = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "        c_t = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            h_t, c_t = self.lstm_cell(X[:, t, :], h_t, c_t)\n",
        "            outputs.append(self.fc(h_t).unsqueeze(1))  # Store projected output\n",
        "         # Concatenate outputs\n",
        "        return torch.cat(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "uUQHyqv2B06J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Data Generation"
      ],
      "metadata": {
        "id": "CQ9A9rXEF0ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are doing the Train/Validation/Test Split"
      ],
      "metadata": {
        "id": "2CgfpgZ44l6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "# Data Generation\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "class CopyTaskDataset(Dataset):\n",
        "    def __init__(self, sequence_length, vocab_size=10, dataset_size=1000):\n",
        "        self.sequence_length = sequence_length  # Length of the sc\n",
        "        self.vocab_size = vocab_size  # NNumber of unique simbols\n",
        "        self.dataset_size = dataset_size  # Number of total examp\n",
        "        self.delimiter_token = vocab_size  # Delimitator\n",
        "        self.blank_token = vocab_size + 1  # Blank space\n",
        "        self.data, self.targets = self.genData()\n",
        "\n",
        "    def genData(self):\n",
        "        data = []\n",
        "        targets = []\n",
        "\n",
        "        for _ in range(self.dataset_size):\n",
        "            # Generate akeatory sequency of int numbers\n",
        "            seq = np.random.randint(0, self.vocab_size, size=self.sequence_length)\n",
        "\n",
        "            # Creating entry delimitator and blank spaces\n",
        "            input_seq = np.concatenate([\n",
        "                seq,  # or sequences\n",
        "                [self.delimiter_token],  # Delimitator\n",
        "                np.full(self.sequence_length, self.blank_token)  # Blank soace\n",
        "            ])\n",
        "\n",
        "            # Expected output\n",
        "            target_seq = np.concatenate([\n",
        "                np.full(self.sequence_length + 1, self.blank_token),  # Ignrar until the delimitator\n",
        "                seq  # Copy org sequence\n",
        "            ])\n",
        "\n",
        "            data.append(input_seq)\n",
        "            targets.append(target_seq)\n",
        "\n",
        "        return torch.tensor(np.array(data), dtype=torch.long), torch.tensor(np.array(targets), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "# ------- Split Dataset into Train, Validation, and Test Sets\n",
        "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.1):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    val_size = int(val_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "# ------- PyTorch DataLoader\n",
        "def get_dataloader(sequence_length, vocab_size=10, dataset_size=5000, batch_size=32):\n",
        "    dataset = CopyTaskDataset(sequence_length, vocab_size, dataset_size)\n",
        "    train_dataset, val_dataset, test_dataset = split_dataset(dataset)\n",
        "    return (DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
        "            DataLoader(val_dataset, batch_size=batch_size, shuffle=False),\n",
        "            DataLoader(test_dataset, batch_size=batch_size, shuffle=False))\n",
        "\n",
        "# ------- Print Examples\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = CopyTaskDataset(sequence_length=5, vocab_size=10, dataset_size=5)\n",
        "    for i in range(2):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(\"Input Sequence: \", dataset.data[i].tolist())\n",
        "        print(\"Target Sequence: \", dataset.targets[i].tolist())\n",
        "        print(\"-\" * 20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmbePXoxDnV2",
        "outputId": "94eb1f00-ff4c-4883-e05c-f5674cd8bfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Input Sequence:  [1, 2, 2, 8, 7, 10, 11, 11, 11, 11, 11]\n",
            "Target Sequence:  [11, 11, 11, 11, 11, 11, 1, 2, 2, 8, 7]\n",
            "--------------------\n",
            "Example 2:\n",
            "Input Sequence:  [1, 3, 3, 4, 5, 10, 11, 11, 11, 11, 11]\n",
            "Target Sequence:  [11, 11, 11, 11, 11, 11, 1, 3, 3, 4, 5]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "2FoY14DYF30t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Training with Hyperparameter Search and Validation"
      ],
      "metadata": {
        "id": "AV2tsgUf9Z6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#------------------------------------------------------\n",
        "# PyTorch Benchmark\n",
        "#---------------------------------------------------\n",
        "\n",
        "def benchmark_pytorch(input_size, hidden_size, train_loader, val_loader, epochs, lr):\n",
        "    model = LSTMPyTorch(input_size, hidden_size, output_size=input_size)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Convert to one hot encoding for inputs\n",
        "            inputs = nn.functional.one_hot(inputs, num_classes=input_size).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Reshape outputs and targets for loss computation\n",
        "            loss = criterion(outputs.view(-1, input_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = outputs.argmax(dim=-1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.numel()\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                inputs = nn.functional.one_hot(inputs, num_classes=input_size).float()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.view(-1, input_size), targets.view(-1))\n",
        "                val_loss += loss.item()\n",
        "                predictions = outputs.argmax(dim=-1)\n",
        "                val_correct += (predictions == targets).sum().item()\n",
        "                val_total += targets.numel()\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    return time.time() - start_time, avg_train_loss, avg_val_loss\n",
        "\n",
        "# ------- Hyperparameter Search\n",
        "def hyperparameter_search(train_loader, val_loader, num_combinations=15):\n",
        "    param_space = {\n",
        "        \"hidden_size\": [64, 128],\n",
        "        \"batch_size\": [16, 32],\n",
        "        \"learning_rate\": [0.001, 0.0005],\n",
        "        \"epochs\": [5],\n",
        "    }\n",
        "\n",
        "    total_combinations = len(param_space[\"hidden_size\"]) * len(param_space[\"batch_size\"]) * len(param_space[\"learning_rate\"]) * len(param_space[\"epochs\"])\n",
        "    num_combinations = min(num_combinations, total_combinations)\n",
        "    sampled_configs = random.sample(\n",
        "        [(h, b, lr, e) for h in param_space[\"hidden_size\"]\n",
        "                      for b in param_space[\"batch_size\"]\n",
        "                      for lr in param_space[\"learning_rate\"]\n",
        "                      for e in param_space[\"epochs\"]], num_combinations)\n",
        "\n",
        "    best_config = None\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for config in sampled_configs:\n",
        "        h_size, b_size, lr, e = config\n",
        "        print(f\"\\nTesting Config: Hidden={h_size}, Batch={b_size}, LR={lr}, Epochs={e}\")\n",
        "\n",
        "        train_loader, val_loader, _ = get_dataloader(sequence_length=100, batch_size=b_size)  # Adjust batch size\n",
        "        training_time, final_loss, _ = benchmark_pytorch(input_size=12, hidden_size=h_size, train_loader=train_loader, val_loader=val_loader, epochs=e, lr=lr)\n",
        "\n",
        "        if final_loss < best_loss:\n",
        "            best_loss = final_loss\n",
        "            best_config = config\n",
        "\n",
        "    print(f\"\\nBest Hyperparameter Configuration: {best_config}\")\n",
        "    return best_config\n",
        "\n",
        "# ------- Run Hyperparameter Search and Training with Validation\n",
        "train_loader, val_loader, _ = get_dataloader(sequence_length=100, batch_size=32)  # Load train/val sets\n",
        "print(\"\\nSearching for Best Hyperparameters...\")\n",
        "best_hyperparams = hyperparameter_search(train_loader, val_loader, num_combinations=15)\n",
        "print(\"\\nBest Hyperparameters Selected. Training final model...\")\n",
        "\n",
        "best_h_size, best_b_size, best_lr, best_epochs = best_hyperparams\n",
        "train_loader, val_loader, _ = get_dataloader(sequence_length=100, batch_size=best_b_size)\n",
        "training_time, _, _ = benchmark_pytorch(input_size=12, hidden_size=best_h_size, train_loader=train_loader, val_loader=val_loader, epochs=best_epochs, lr=best_lr)\n",
        "print(f\"\\nFinal Training completed in {training_time:.2f} seconds!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXb02f7DOgXz",
        "outputId": "87ee8445-b123-4e27-f8a7-7781a253fbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Searching for Best Hyperparameters...\n",
            "\n",
            "Testing Config: Hidden=64, Batch=16, LR=0.001, Epochs=5\n",
            "Epoch 1/5, Train Loss: 1.3911, Train Accuracy: 0.5216, Val Loss: 1.1699, Val Accuracy: 0.5510\n",
            "Epoch 2/5, Train Loss: 1.1618, Train Accuracy: 0.5520, Val Loss: 1.1572, Val Accuracy: 0.5511\n",
            "Epoch 3/5, Train Loss: 1.1546, Train Accuracy: 0.5521, Val Loss: 1.1527, Val Accuracy: 0.5523\n",
            "Epoch 4/5, Train Loss: 1.1693, Train Accuracy: 0.5507, Val Loss: 1.1515, Val Accuracy: 0.5523\n",
            "Epoch 5/5, Train Loss: 1.1505, Train Accuracy: 0.5521, Val Loss: 1.1497, Val Accuracy: 0.5521\n",
            "\n",
            "Testing Config: Hidden=64, Batch=32, LR=0.0005, Epochs=5\n",
            "Epoch 1/5, Train Loss: 1.8487, Train Accuracy: 0.4591, Val Loss: 1.3190, Val Accuracy: 0.5499\n",
            "Epoch 2/5, Train Loss: 1.2675, Train Accuracy: 0.5499, Val Loss: 1.2338, Val Accuracy: 0.5523\n",
            "Epoch 3/5, Train Loss: 1.2115, Train Accuracy: 0.5509, Val Loss: 1.2018, Val Accuracy: 0.5511\n",
            "Epoch 4/5, Train Loss: 1.1864, Train Accuracy: 0.5513, Val Loss: 1.1757, Val Accuracy: 0.5516\n",
            "Epoch 5/5, Train Loss: 1.1846, Train Accuracy: 0.5512, Val Loss: 1.1742, Val Accuracy: 0.5494\n",
            "\n",
            "Testing Config: Hidden=128, Batch=32, LR=0.001, Epochs=5\n",
            "Epoch 1/5, Train Loss: 1.4675, Train Accuracy: 0.5167, Val Loss: 1.1912, Val Accuracy: 0.5506\n",
            "Epoch 2/5, Train Loss: 1.1715, Train Accuracy: 0.5517, Val Loss: 1.1637, Val Accuracy: 0.5516\n",
            "Epoch 3/5, Train Loss: 1.2453, Train Accuracy: 0.5475, Val Loss: 1.1800, Val Accuracy: 0.5510\n",
            "Epoch 4/5, Train Loss: 1.1672, Train Accuracy: 0.5513, Val Loss: 1.1604, Val Accuracy: 0.5524\n",
            "Epoch 5/5, Train Loss: 1.1579, Train Accuracy: 0.5519, Val Loss: 1.1560, Val Accuracy: 0.5510\n",
            "\n",
            "Testing Config: Hidden=128, Batch=16, LR=0.001, Epochs=5\n",
            "Epoch 1/5, Train Loss: 1.3307, Train Accuracy: 0.5335, Val Loss: 1.1719, Val Accuracy: 0.5502\n",
            "Epoch 2/5, Train Loss: 1.1608, Train Accuracy: 0.5521, Val Loss: 1.1555, Val Accuracy: 0.5524\n",
            "Epoch 3/5, Train Loss: 1.1535, Train Accuracy: 0.5520, Val Loss: 1.1518, Val Accuracy: 0.5519\n",
            "Epoch 4/5, Train Loss: 1.1508, Train Accuracy: 0.5521, Val Loss: 1.1496, Val Accuracy: 0.5532\n",
            "Epoch 5/5, Train Loss: 1.1491, Train Accuracy: 0.5524, Val Loss: 1.1484, Val Accuracy: 0.5539\n",
            "\n",
            "Testing Config: Hidden=64, Batch=16, LR=0.0005, Epochs=5\n",
            "Epoch 1/5, Train Loss: 1.5573, Train Accuracy: 0.5001, Val Loss: 1.2418, Val Accuracy: 0.5499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation Loss Plots"
      ],
      "metadata": {
        "id": "Ib11yC3fOx4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluation"
      ],
      "metadata": {
        "id": "kD2E-WzBVpMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import sem\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ------- Model Evaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(model.fc.weight.device), batch_y.to(model.fc.weight.device)\n",
        "            batch_x = nn.functional.one_hot(batch_x, num_classes=model.fc.out_features).float()\n",
        "\n",
        "            predictions = model(batch_x)\n",
        "            loss = loss_fn(predictions.view(-1, model.fc.out_features), batch_y.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted_labels = predictions.argmax(dim=-1)\n",
        "            correct_predictions = (predicted_labels == batch_y).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += batch_y.numel()\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ------- Evaluation on Different T Values\n",
        "def full_evaluation(model, dataset_class, sequence_lengths=[200, 500, 1000], trials=3):\n",
        "    results = {}\n",
        "    print(\"\\nEvaluating best model on different T values...\")\n",
        "\n",
        "    trial_accuracies = {T: [] for T in sequence_lengths}\n",
        "\n",
        "    for T in sequence_lengths:\n",
        "        print(f\"Testing on T={T}...\")\n",
        "        _, _, test_loader = get_dataloader(sequence_length=T, batch_size=best_b_size)\n",
        "\n",
        "        for _ in range(trials):\n",
        "            loss, accuracy = evaluate_model(model, test_loader)\n",
        "            trial_accuracies[T].append(accuracy)\n",
        "\n",
        "    # Compute mean and standard error\n",
        "    results = {\n",
        "        T: {\n",
        "            \"mean_accuracy\": np.mean(trial_accuracies[T]),\n",
        "            \"std_error\": sem(trial_accuracies[T])\n",
        "        } for T in sequence_lengths\n",
        "    }\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    for T, metrics in results.items():\n",
        "        print(f\"T={T}: Accuracy={metrics['mean_accuracy']:.4f} ± {metrics['std_error']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------- Run Evaluation\n",
        "print(\"\\nEvaluating the best model...\")\n",
        "_, _, test_loader = get_dataloader(sequence_length=100, batch_size=best_b_size)\n",
        "evaluation_results = full_evaluation(\n",
        "    LSTMPyTorch(input_size=12, hidden_size=best_h_size, output_size=12),\n",
        "    CopyTaskDataset\n",
        ")\n",
        "print(\"\\nEvaluation completed!\")"
      ],
      "metadata": {
        "id": "HGd6zZIFVsfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Multiplicative LSTM"
      ],
      "metadata": {
        "id": "EebWef0dX7Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Implementation of the model"
      ],
      "metadata": {
        "id": "i3JSxTC_Z0hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#--------------------------------------------------------\n",
        "# PyTorch Implementation of Multiplicative LSTM\n",
        "#------------------------------------------------------\n",
        "\n",
        "# ------- Single-Step Multiplicative LSTM Cell\n",
        "class MLSTMCellPyTorch(nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step Multiplicative LSTM cell in PyTorch\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Multiplicative transformation\n",
        "        self.W_m = nn.Linear(input_size + hidden_size, input_size)\n",
        "\n",
        "        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)  # Input gate\n",
        "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)  # Forget gate\n",
        "        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)  # Output gate\n",
        "        self.W_c = nn.Linear(input_size + hidden_size, hidden_size)  # Candidate cell state\n",
        "\n",
        "    def forward(self, x_t, h_prev, c_prev):\n",
        "        \"\"\"\n",
        "        x_t: [batch_size, input_size]\n",
        "        h_prev: [batch_size, hidden_size]\n",
        "        c_prev: [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        # Compute multiplicative transformation\n",
        "        m_t = torch.sigmoid(self.W_m(torch.cat((x_t, h_prev), dim=1)))\n",
        "        x_t = x_t * m_t\n",
        "\n",
        "        combined = torch.cat((x_t, h_prev), dim=1)\n",
        "\n",
        "        i_t = torch.sigmoid(self.W_i(combined))\n",
        "        f_t = torch.sigmoid(self.W_f(combined))\n",
        "        o_t = torch.sigmoid(self.W_o(combined))\n",
        "        c_tilde = torch.tanh(self.W_c(combined))\n",
        "\n",
        "        c_t = f_t * c_prev + i_t * c_tilde  # Cell state update\n",
        "        h_t = o_t * torch.tanh(c_t)  # Hidden state update\n",
        "\n",
        "        return h_t, c_t\n",
        "\n",
        "# ------- Higher-level PyTorch MLSTM that unrolls over time\n",
        "class MLSTMPyTorch(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = MLSTMCellPyTorch(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # Output projection\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        X: [batch_size, seq_length, input_size]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        h_t = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "        c_t = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            h_t, c_t = self.lstm_cell(X[:, t, :], h_t, c_t)\n",
        "            outputs.append(self.fc(h_t).unsqueeze(1))\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "azp7dllFZkSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Training"
      ],
      "metadata": {
        "id": "ZTW1VVsmZ3FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#---------------------------------------------------\n",
        "# PyTorch Benchmark for MLSTM Training with Validation\n",
        "#---------------------------------------------------\n",
        "\n",
        "def benchmark_pytorch(input_size, hidden_size, train_loader, val_loader, epochs, lr):\n",
        "    model = MLSTMPyTorch(input_size, hidden_size, output_size=input_size)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Convert to one-hot encoding for inputs\n",
        "            inputs = nn.functional.one_hot(inputs, num_classes=input_size).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Reshape outputs and targets for loss computation\n",
        "            loss = criterion(outputs.view(-1, input_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = outputs.argmax(dim=-1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.numel()\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                inputs = nn.functional.one_hot(inputs, num_classes=input_size).float()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.view(-1, input_size), targets.view(-1))\n",
        "                val_loss += loss.item()\n",
        "                predictions = outputs.argmax(dim=-1)\n",
        "                val_correct += (predictions == targets).sum().item()\n",
        "                val_total += targets.numel()\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    return time.time() - start_time, avg_train_loss, avg_val_loss\n",
        "\n",
        "# ------- Run Benchmark for MLSTM with Best Hyperparameters\n",
        "train_loader, val_loader, _ = get_dataloader(sequence_length=100, batch_size=best_b_size)\n",
        "print(\"\\nBenchmarking MLSTM with Best Hyperparameters...\")\n",
        "\n",
        "training_time, _, _ = benchmark_pytorch(\n",
        "    input_size=12,\n",
        "    hidden_size=best_h_size,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=best_epochs,\n",
        "    lr=best_lr\n",
        ")\n",
        "\n",
        "print(f\"\\nMLSTM Training completed in {training_time:.2f} seconds!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFWosUmWbeWm",
        "outputId": "ba380efe-114d-4b78-ff9b-288891c1a11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Benchmarking MLSTM with Best Hyperparameters...\n",
            "Epoch 1/5, Train Loss: 1.8947, Train Accuracy: 0.4937, Val Loss: 1.4141, Val Accuracy: 0.5025\n",
            "Epoch 2/5, Train Loss: 1.3102, Train Accuracy: 0.5292, Val Loss: 1.2978, Val Accuracy: 0.5507\n",
            "Epoch 3/5, Train Loss: 1.2502, Train Accuracy: 0.5490, Val Loss: 1.1993, Val Accuracy: 0.5535\n",
            "Epoch 4/5, Train Loss: 1.1886, Train Accuracy: 0.5507, Val Loss: 1.1800, Val Accuracy: 0.5523\n",
            "Epoch 5/5, Train Loss: 1.1755, Train Accuracy: 0.5531, Val Loss: 1.1713, Val Accuracy: 0.5510\n",
            "\n",
            "MLSTM Training completed in 80.46 seconds!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation Loss Plots"
      ],
      "metadata": {
        "id": "SNwkO703O0v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, best_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(range(1, best_epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, best_epochs + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
        "plt.plot(range(1, best_epochs + 1), val_accuracies, label='Validation Accuracy', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7hd0Gfo7POWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "DGcembBHblUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import sem\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ------- Model Evaluation for MLSTM\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(model.fc.weight.device), batch_y.to(model.fc.weight.device)\n",
        "            batch_x = nn.functional.one_hot(batch_x, num_classes=model.fc.out_features).float()\n",
        "\n",
        "            predictions = model(batch_x)\n",
        "            loss = loss_fn(predictions.view(-1, model.fc.out_features), batch_y.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted_labels = predictions.argmax(dim=-1)\n",
        "            correct_predictions = (predicted_labels == batch_y).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += batch_y.numel()\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ------- Full Evaluation on Different T Values for MLSTM\n",
        "def full_evaluation(model, dataset_class, sequence_lengths=[200, 500, 1000], trials=3):\n",
        "    results = {}\n",
        "    print(\"\\nEvaluating best MLSTM model on different T values...\")\n",
        "\n",
        "    trial_accuracies = {T: [] for T in sequence_lengths}\n",
        "\n",
        "    for T in sequence_lengths:\n",
        "        print(f\"Testing on T={T}...\")\n",
        "        _, _, test_loader = get_dataloader(sequence_length=T, batch_size=best_b_size)\n",
        "\n",
        "        for _ in range(trials):\n",
        "            loss, accuracy = evaluate_model(model, test_loader)\n",
        "            trial_accuracies[T].append(accuracy)\n",
        "\n",
        "    # Compute mean and standard error\n",
        "    results = {\n",
        "        T: {\n",
        "            \"mean_accuracy\": np.mean(trial_accuracies[T]),\n",
        "            \"std_error\": sem(trial_accuracies[T])\n",
        "        } for T in sequence_lengths\n",
        "    }\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    for T, metrics in results.items():\n",
        "        print(f\"T={T}: Accuracy={metrics['mean_accuracy']:.4f} ± {metrics['std_error']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------- Run Evaluation for MLSTM\n",
        "print(\"\\nEvaluating the best MLSTM model...\")\n",
        "_, _, test_loader = get_dataloader(sequence_length=100, batch_size=best_b_size)\n",
        "evaluation_results = full_evaluation(\n",
        "    MLSTMPyTorch(input_size=12, hidden_size=best_h_size, output_size=12),\n",
        "    CopyTaskDataset\n",
        ")\n",
        "print(\"\\nEvaluation completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEQD4cdyeLiz",
        "outputId": "30144fc3-90d8-4b44-bae4-b622a76cdcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the best MLSTM model...\n",
            "\n",
            "Evaluating best MLSTM model on different T values...\n",
            "Testing on T=200...\n",
            "Testing on T=500...\n",
            "Testing on T=1000...\n",
            "\n",
            "Evaluation Results:\n",
            "T=200: Accuracy=0.0487 ± 0.0000\n",
            "T=500: Accuracy=0.0505 ± 0.0000\n",
            "T=1000: Accuracy=0.0507 ± 0.0000\n",
            "\n",
            "Evaluation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUPj8Z4o_2i"
      },
      "source": [
        "# Standard GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Implementation of the model"
      ],
      "metadata": {
        "id": "bi5pGgB5x5Ks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko-q6fOIpAzf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#----------------------------------------\n",
        "# PyTorch Implementation of Standard GRU\n",
        "#----------------------------------------\n",
        "\n",
        "# ------- Single-Step GRU Cell\n",
        "class GRUCellPyTorch(nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step GRU cell in PyTorch.\n",
        "    Implements the standard GRU equations from the provided document.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "\n",
        "        self.W_z = nn.Linear(input_size, hidden_size)  # Update gate weights for input\n",
        "        self.U_z = nn.Linear(hidden_size, hidden_size)  # Update gate weights for hidden state\n",
        "        self.W_r = nn.Linear(input_size, hidden_size)  # Reset gate weights for input\n",
        "        self.U_r = nn.Linear(hidden_size, hidden_size)  # Reset gate weights for hidden state\n",
        "        self.W_h = nn.Linear(input_size, hidden_size)  # Candidate hidden state weights for input\n",
        "        self.U_h = nn.Linear(hidden_size, hidden_size)  # Candidate hidden state weights for hidden state\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        \"\"\"\n",
        "        x_t: [batch_size, input_size]\n",
        "        h_prev: [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h_prev))\n",
        "        r_t = torch.sigmoid(self.W_r(x_t) + self.U_r(h_prev))\n",
        "        h_tilde = torch.tanh(self.W_h(x_t) + self.U_h(r_t * h_prev))\n",
        "\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_tilde\n",
        "\n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level PyTorch GRU that unrolls over time\n",
        "class GRUPyTorch(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = GRUCellPyTorch(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        X: [batch_size, seq_length, input_size]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        h_t = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            h_t = self.gru_cell(X[:, t, :], h_t)\n",
        "            outputs.append(self.fc(h_t).unsqueeze(1))\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Training"
      ],
      "metadata": {
        "id": "JTWR4wsrzu1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with Validation"
      ],
      "metadata": {
        "id": "5LAmDwd7-X5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "########################################\n",
        "# PyTorch Benchmark Standard GRU\n",
        "########################################\n",
        "\n",
        "def benchmark_pytorch(input_size, hidden_size, train_loader, val_loader, epochs, lr):\n",
        "    model = GRUPyTorch(input_size, hidden_size, output_size=input_size)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Convert to one-hot encoding for inputs\n",
        "            inputs = nn.functional.one_hot(inputs, num_classes=input_size).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Reshape outputs and targets for loss computation\n",
        "            loss = criterion(outputs.view(-1, input_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = outputs.argmax(dim=-1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.numel()\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                inputs = nn.functional.one_hot(inputs, num_classes=input_size).float()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.view(-1, input_size), targets.view(-1))\n",
        "                val_loss += loss.item()\n",
        "                predictions = outputs.argmax(dim=-1)\n",
        "                val_correct += (predictions == targets).sum().item()\n",
        "                val_total += targets.numel()\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    return time.time() - start_time, avg_train_loss, avg_val_loss\n",
        "\n",
        "# ------- Run Benchmark for Standard GRU with Best Hyperparameters\n",
        "train_loader, val_loader, _ = get_dataloader(sequence_length=100, batch_size=best_b_size)\n",
        "print(\"\\nBenchmarking Standard GRU with Best Hyperparameters...\")\n",
        "\n",
        "training_time, _, _ = benchmark_pytorch(\n",
        "    input_size=12,\n",
        "    hidden_size=best_h_size,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=best_epochs,\n",
        "    lr=best_lr\n",
        ")\n",
        "\n",
        "print(f\"\\nStandard GRU Training completed in {training_time:.2f} seconds!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy-NvqZkzwmU",
        "outputId": "f6fa4f19-f6c5-40d2-a9f6-2a214a3d18f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Benchmarking Standard GRU with Best Hyperparameters...\n",
            "Epoch 1/5, Loss: 1.2897, Accuracy: 0.5379\n",
            "Epoch 2/5, Loss: 1.1551, Accuracy: 0.5519\n",
            "Epoch 3/5, Loss: 1.1548, Accuracy: 0.5522\n",
            "Epoch 4/5, Loss: 1.1492, Accuracy: 0.5521\n",
            "Epoch 5/5, Loss: 1.1478, Accuracy: 0.5521\n",
            "\n",
            "Standard GRU Training completed in 294.35 seconds!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Validation Loss Plots"
      ],
      "metadata": {
        "id": "Wm1TTWulO5Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, best_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(range(1, best_epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, best_epochs + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
        "plt.plot(range(1, best_epochs + 1), val_accuracies, label='Validation Accuracy', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vtjqz2_GPY4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluation"
      ],
      "metadata": {
        "id": "yrYOKaED2CRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import sem\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ------- Model Evaluation for Standard GRU\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(model.fc.weight.device), batch_y.to(model.fc.weight.device)\n",
        "            batch_x = nn.functional.one_hot(batch_x, num_classes=model.fc.out_features).float()\n",
        "\n",
        "            predictions = model(batch_x)\n",
        "            loss = loss_fn(predictions.view(-1, model.fc.out_features), batch_y.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted_labels = predictions.argmax(dim=-1)\n",
        "            correct_predictions = (predicted_labels == batch_y).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += batch_y.numel()\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ------- Full Evaluation on Different T Values for Standard GRU\n",
        "def full_evaluation(model, dataset_class, sequence_lengths=[200, 500, 1000], trials=3):\n",
        "    results = {}\n",
        "    print(\"\\nEvaluating best Standard GRU model on different T values...\")\n",
        "\n",
        "    trial_accuracies = {T: [] for T in sequence_lengths}\n",
        "\n",
        "    for T in sequence_lengths:\n",
        "        print(f\"Testing on T={T}...\")\n",
        "        _, _, test_loader = get_dataloader(sequence_length=T, batch_size=best_b_size)\n",
        "\n",
        "        for _ in range(trials):\n",
        "            loss, accuracy = evaluate_model(model, test_loader)\n",
        "            trial_accuracies[T].append(accuracy)\n",
        "\n",
        "    # Compute mean and standard error\n",
        "    results = {\n",
        "        T: {\n",
        "            \"mean_accuracy\": np.mean(trial_accuracies[T]),\n",
        "            \"std_error\": sem(trial_accuracies[T])\n",
        "        } for T in sequence_lengths\n",
        "    }\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    for T, metrics in results.items():\n",
        "        print(f\"T={T}: Accuracy={metrics['mean_accuracy']:.4f} ± {metrics['std_error']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------- Run Evaluation for Standard GRU\n",
        "print(\"\\nEvaluating the best Standard GRU model...\")\n",
        "_, _, test_loader = get_dataloader(sequence_length=100, batch_size=best_b_size)\n",
        "evaluation_results = full_evaluation(\n",
        "    GRUPyTorch(input_size=12, hidden_size=best_h_size, output_size=12),\n",
        "    CopyTaskDataset\n",
        ")\n",
        "print(\"\\nEvaluation completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGd5nxsZ2EmY",
        "outputId": "c8985621-9534-4669-80bb-7cdfb7e3c1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the best Standard GRU model...\n",
            "\n",
            "Evaluating best Standard GRU model on different T values...\n",
            "Testing on T=200...\n",
            "Testing on T=500...\n",
            "Testing on T=1000...\n",
            "\n",
            "Evaluation Results:\n",
            "T=200: Accuracy=0.0499 ± 0.0001\n",
            "T=500: Accuracy=0.0499 ± 0.0002\n",
            "T=1000: Accuracy=0.0500 ± 0.0001\n",
            "\n",
            "Evaluation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxRL5virpMTM"
      },
      "source": [
        "# Multiplicative GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Implementation of the model"
      ],
      "metadata": {
        "id": "PvretVKE433B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtASa2PwpOGX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#----------------------------------------------\n",
        "# PyTorch Implementation of Multiplicative GRU\n",
        "#----------------------------------------------\n",
        "\n",
        "# ------- Single Step Multiplicative GRU Cell\n",
        "class MGRUCellPyTorch(nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step Multiplicative GRU cell in PyTorch.\n",
        "    Implements the MGRU equations from the provided document.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Multiplicative transformation\n",
        "        self.W_m = nn.Linear(input_size + hidden_size, input_size)\n",
        "\n",
        "        self.W_z = nn.Linear(input_size, hidden_size)  # Update gate weights for input\n",
        "        self.U_z = nn.Linear(hidden_size, hidden_size)  # Update gate weights for hidden state\n",
        "        self.W_r = nn.Linear(input_size, hidden_size)  # Reset gate weights for input\n",
        "        self.U_r = nn.Linear(hidden_size, hidden_size)  # Reset gate weights for hidden state\n",
        "        self.W_h = nn.Linear(input_size, hidden_size)  # Candidate hidden state weights for input\n",
        "        self.U_h = nn.Linear(hidden_size, hidden_size)  # Candidate hidden state weights for hidden state\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        \"\"\"\n",
        "        x_t: [batch_size, input_size]\n",
        "        h_prev: [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        # Compute multiplicative transformation\n",
        "        m_t = torch.sigmoid(self.W_m(torch.cat((x_t, h_prev), dim=1)))\n",
        "        x_t = x_t * m_t\n",
        "\n",
        "        z_t = torch.sigmoid(self.W_z(x_t) + self.U_z(h_prev))\n",
        "        r_t = torch.sigmoid(self.W_r(x_t) + self.U_r(h_prev))\n",
        "        h_tilde = torch.tanh(self.W_h(x_t) + self.U_h(r_t * h_prev))\n",
        "\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_tilde\n",
        "\n",
        "        return h_t\n",
        "\n",
        "# ------- Higher-level PyTorch MGRU that unrolls over time\n",
        "class MGRUPyTorch(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru_cell = MGRUCellPyTorch(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        X: [batch_size, seq_length, input_size]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        h_t = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            h_t = self.gru_cell(X[:, t, :], h_t)\n",
        "            outputs.append(self.fc(h_t).unsqueeze(1))\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Training"
      ],
      "metadata": {
        "id": "KF_WC9oH49xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with Validation"
      ],
      "metadata": {
        "id": "jFrt8zzH-xBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#------------------------------------------\n",
        "# PyTorch Benchmark for Multiplicative GRU\n",
        "#-----------------------------------------\n",
        "\n",
        "def benchmark_pytorch(input_size, hidden_size, train_loader, val_loader, epochs, lr):\n",
        "    model = MGRUPyTorch(input_size, hidden_size, output_size=input_size)  # Ensure output matches input size\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Convert to one-hot encoding for inputs\n",
        "            inputs = nn.functional.one_hot(inputs, num_classes=input_size).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Reshape outputs and targets for loss computation\n",
        "            loss = criterion(outputs.view(-1, input_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = outputs.argmax(dim=-1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.numel()\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                inputs = nn.functional.one_hot(inputs, num_classes=input_size).float()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.view(-1, input_size), targets.view(-1))\n",
        "                val_loss += loss.item()\n",
        "                predictions = outputs.argmax(dim=-1)\n",
        "                val_correct += (predictions == targets).sum().item()\n",
        "                val_total += targets.numel()\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    return time.time() - start_time, avg_train_loss, avg_val_loss\n",
        "\n",
        "# ------- Run Benchmark for Multiplicative GRU with Best Hyperparameters -------\n",
        "train_loader, val_loader, _ = get_dataloader(sequence_length=100, batch_size=best_b_size)  # Use best batch size\n",
        "print(\"\\nBenchmarking Multiplicative GRU with Best Hyperparameters...\")\n",
        "\n",
        "training_time, _, _ = benchmark_pytorch(\n",
        "    input_size=12,\n",
        "    hidden_size=best_h_size,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=best_epochs,\n",
        "    lr=best_lr\n",
        ")\n",
        "\n",
        "print(f\"\\nMultiplicative GRU Training completed in {training_time:.2f} seconds!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mli1aWy_4_14",
        "outputId": "d0c7cb4f-9c18-4426-ddb9-8ddb90088fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Benchmarking Multiplicative GRU with Best Hyperparameters...\n",
            "Epoch 1/5, Loss: 1.3090, Accuracy: 0.5418\n",
            "Epoch 2/5, Loss: 1.1563, Accuracy: 0.5520\n",
            "Epoch 3/5, Loss: 1.1513, Accuracy: 0.5523\n",
            "Epoch 4/5, Loss: 1.1490, Accuracy: 0.5522\n",
            "Epoch 5/5, Loss: 1.1484, Accuracy: 0.5524\n",
            "\n",
            "Multiplicative GRU Training completed in 350.46 seconds!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation Loss Plots"
      ],
      "metadata": {
        "id": "CqA4HNESO7Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, best_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(range(1, best_epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, best_epochs + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
        "plt.plot(range(1, best_epochs + 1), val_accuracies, label='Validation Accuracy', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PCRQlfyhPcEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "imkEqbvK5Uon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import sem\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ------- Model Evaluation for Multiplicative GRU\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(model.fc.weight.device), batch_y.to(model.fc.weight.device)\n",
        "            batch_x = nn.functional.one_hot(batch_x, num_classes=model.fc.out_features).float()\n",
        "\n",
        "            predictions = model(batch_x)\n",
        "            loss = loss_fn(predictions.view(-1, model.fc.out_features), batch_y.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted_labels = predictions.argmax(dim=-1)\n",
        "            correct_predictions = (predicted_labels == batch_y).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += batch_y.numel()\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# ------- Full Evaluation on Different T Values for Multiplicative GRU\n",
        "def full_evaluation(model, dataset_class, sequence_lengths=[100, 200, 500, 1000], trials=3):\n",
        "    results = {}\n",
        "    print(\"\\nEvaluating best Multiplicative GRU model on different T values...\")\n",
        "\n",
        "    trial_accuracies = {T: [] for T in sequence_lengths}\n",
        "\n",
        "    for T in sequence_lengths:\n",
        "        print(f\"Testing on T={T}...\")\n",
        "        _, _, test_loader = get_dataloader(sequence_length=T, batch_size=best_b_size)\n",
        "\n",
        "        for _ in range(trials):\n",
        "            loss, accuracy = evaluate_model(model, test_loader)\n",
        "            trial_accuracies[T].append(accuracy)\n",
        "\n",
        "    # Compute mean and standard error\n",
        "    results = {\n",
        "        T: {\n",
        "            \"mean_accuracy\": np.mean(trial_accuracies[T]),\n",
        "            \"std_error\": sem(trial_accuracies[T])\n",
        "        } for T in sequence_lengths\n",
        "    }\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    for T, metrics in results.items():\n",
        "        print(f\"T={T}: Accuracy={metrics['mean_accuracy']:.4f} ± {metrics['std_error']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------- Run Evaluation for Multiplicative GRU -------\n",
        "print(\"\\nEvaluating the best Multiplicative GRU model...\")\n",
        "_, _, test_loader = get_dataloader(sequence_length=100, batch_size=best_b_size)\n",
        "evaluation_results = full_evaluation(\n",
        "    MGRUPyTorch(input_size=12, hidden_size=best_h_size, output_size=12),\n",
        "    CopyTaskDataset\n",
        ")\n",
        "print(\"\\nEvaluation completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpL-qQeb5Wfh",
        "outputId": "9d3bf1b2-94a1-4fc3-e03c-2af0a0f0515c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the best Multiplicative GRU model...\n",
            "\n",
            "Evaluating best Multiplicative GRU model on different T values...\n",
            "Testing on T=200...\n",
            "Testing on T=500...\n",
            "Testing on T=1000...\n",
            "\n",
            "Evaluation Results:\n",
            "T=200: Accuracy=0.0499 ± 0.0001\n",
            "T=500: Accuracy=0.0498 ± 0.0001\n",
            "T=1000: Accuracy=0.0498 ± 0.0000\n",
            "\n",
            "Evaluation completed!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m03_Eo4PBeOM",
        "9ASGGgDfBYlk",
        "kD2E-WzBVpMZ",
        "EebWef0dX7Lw",
        "i3JSxTC_Z0hB",
        "jZUPj8Z4o_2i",
        "nxRL5virpMTM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}